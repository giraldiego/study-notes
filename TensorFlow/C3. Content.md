# C3 - Natural Language Processing in TensorFlow

## Week 1

>The first step in understanding sentiment in text, and in particular when training a neural network to do so is the tokenization of that text. This is the process of converting the text into numeric values, with a number representing a word or a character. This week you'll learn about the Tokenizer and pad_sequences APIs in TensorFlow and how they can be used to prepare and encode text and sentences to get them ready for training neural networks!

### Content
- Word based encodings
- Using APIs
- Text to sequence

### Tokenizer

`from tensorflow.keras.preprocessing.text import Tokenizer`

The code takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method and you can get the result by looking at the `word_index` property. More frequent words have a lower index.

The `num_words` parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. The important thing to note is it does not affect how the `word_index` dictionary is generated.   

Also notice that by default, all punctuation is ignored and words are converted to lower case. You can override these behaviors by modifying the `filters` and `lower` arguments of the `Tokenizer` class as described [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). You can try modifying these in the next cell below and compare the output to the one generated above.

You can define an `oov_token` when the `Tokenizer` is initialized. This will be used when you have input words that are not found in the `word_index` dictionary. For example, you may decide to collect more text after your initial training and decide to not re-generate the `word_index`. 

### Generating Sequences and Padding

You can use then use the result to convert each of the input sentences into a sequence of tokens. That is done using the [`texts_to_sequences()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences)

`from tensorflow.keras.preprocessing.sequence import pad_sequences`

As mentioned in the lecture, you will usually need to pad the sequences into a uniform length because that is what your model expects. You can use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) for that. By default, it will pad according to the length of the longest sequence. You can override this with the `maxlen` argument to define a specific length. Feel free to play with the [other arguments](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences#args) shown in class and compare the result.

```python
padding='post' # default 'pre'
truncating='post' # default 'pre'
```


### Assignment 

- Explore the BBC News Archive _(C3W1_Assignment.ipynb)_

- Removing Stopwords
- Parse csv files

### Ungraded Labs

1. [Simple Tokenizing _(C3_W1_Lab_1_tokenize_basic.ipynb)_](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb)
2. [Simple Sequences _(C3_W1_Lab_2_sequences_basic.ipynb)_](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_2_sequences_basic.ipynb)
3. [Sarcasm _(C3_W1_Lab_3_sarcasm.ipynb)_](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_3_sarcasm.ipynb)

## Week 2 - Word Embeddings

Last week you saw how to use the Tokenizer to prepare your text to be used by a neural network by converting words into numeric tokens, and sequencing sentences from these tokens. This week you'll learn about Embeddings, where these tokens are mapped as vectors in a high dimension space. With Embeddings and labelled examples, these vectors can then be tuned so that words with similar meaning will have a similar direction in the vector space. This will begin the process of training a neural network to understand sentiment in text -- and you'll begin by looking at movie reviews, training a neural network on texts that are labelled 'positive' or 'negative' and determining which words in a sentence drive those meanings.

## Week 2 Wrap up

Here are the key takeaways for this week:

-   You looked at taking your tokenized words and passing them to an Embedding layer.
    
-   Embeddings map your vocabulary to vectors in higher-dimensional space.
    
-   The semantics of the words were learned when those words were labeled with similar meanings. For example, when looking at movie reviews, those movies with positive sentiment had the dimensionality of their words ending up pointing a particular way, and those with negative sentiment pointing in a different direction. From these, the words in future reviews could have their direction established and your model can infer the sentiment from it.
    
-   You then looked at subword tokenization and saw that not only do the meanings of the words matter but also the sequence in which they are found.

### Content
- Word Embeddings
- IMDB Dataset


### Assignment 

- Categorizing the BBC News Archive _(C3W2_Assignment.ipynb)_

### Ungraded Labs

1. Positive or Negative IMDB Reviews _(C3_W2_Lab_1_imdb.ipynb)_

Use [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) to load the dataset

3. Sarcasm Classifier _(C3_W2_Lab_2_sarcasm_classifier.ipynb)_
4. IMDB Review Subwords _(C3_W2_Lab_3_imdb_subwords.ipynb)_

## Week 3 - Sequence Models
> In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'. This week you'll start digging into a variety of model formats that are used in training models to understand context in sequence!


### Assignment 

- Exploring Overfitting in NLP _(C3W3_Assignment.ipynb)_

### Ungraded Labs

1. IMDB Subwords 8K with Single Layer LSTM _(C3_W3_Lab_1_single_layer_LSTM.ipynb)_
2. IMDB Subwords 8K with Multi Layer LSTM _(C3_W3_Lab_2_multiple_layer_LSTM.ipynb)_
3. IMDB Subwords 8K with 1D Convolutional Layer _(C3_W3_Lab_3_Conv1D.ipynb)_
4. IMDB Reviews with GRU (and optional LSTM and Conv1D) _(C3_W3_Lab_4_imdb_reviews_with_GRU_LSTM_Conv1D.ipynb)_
5. Sarcasm with Bidirectional LSTM _(C3_W3_Lab_5_sarcasm_with_bi_LSTM.ipynb)_
6. Sarcasm with 1D Convolutional Layer _(C3_W3_Lab_6_sarcasm_with_1D_convolutional.ipynb)_

## Week 4

### Assignment

- Writing Shakespeare with LSTMs _(C3W4_Assignment.ipynb)_

### Ungraded Labs

1. NLP with Irish Music _(C3_W4_Lab_1.ipynb)_
2. Generating Poetry from Irish Lyrics _(C3_W4_Lab_2_irish_lyrics.ipynb)_
